# -*- coding: utf-8 -*-
"""Data_Generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BNKwIlwsEQILOl9IhUMw7E6E75akLGOW
"""

# Medieval Spanish/Catalan Text Annotation Pipeline for Google Colab
# Enhanced version with robust text cleaning and annotation generation

# =============================================================================
# STEP 1: INSTALLATION AND SETUP (FIXED VERSION)
# =============================================================================

# Method 1: Try installing without force-reinstall first
try:
    !pip install lxml google-generativeai PyPDF2 backoff --quiet
    print("âœ“ Packages installed successfully")
except Exception as e:
    print(f"Standard installation failed: {e}")
    print("Trying alternative installation...")

    # Method 2: Install with specific version constraints
    !pip install --upgrade --no-deps lxml
    !pip install google-generativeai==0.8.5 --quiet
    !pip install PyPDF2==3.0.1 --quiet
    !pip install backoff==2.2.1 --quiet

# For pandas, use the existing version to avoid conflicts
try:
    import pandas as pd
    print(f"âœ“ Using existing pandas version: {pd.__version__}")
except ImportError:
    !pip install pandas --quiet

# Clear proxy settings
!unset HTTP_PROXY HTTPS_PROXY 2>/dev/null || true

# Import required libraries
import os
import re
import time
import json
import sys
import traceback
import pandas as pd
from typing import List, Dict, Optional, Tuple, Any
import xml.etree.ElementTree as ET
from xml.dom import minidom
import PyPDF2
import google.generativeai as genai
from google.colab import drive, userdata
import backoff
from dataclasses import dataclass
from collections import Counter

# =============================================================================
# STEP 2: MOUNT DRIVE AND CONFIGURE API
# =============================================================================

print("ğŸš€ Starting Medieval Text Annotation Pipeline")
print("=" * 60)

# Mount Google Drive
print("\nğŸ“ Mounting Google Drive...")
drive.mount('/content/drive')
print("âœ“ Google Drive mounted successfully")

# Configure Gemini API
try:
    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY not found in Colab secrets")

    genai.configure(api_key=GEMINI_API_KEY)

    # Test API connection
    model = genai.GenerativeModel('gemini-1.5-flash')
    test_response = model.generate_content("Test connection. Respond with: OK")
    print("âœ“ Gemini API configured and tested successfully")

except Exception as e:
    print(f"âŒ Error configuring Gemini API: {e}")
    print("Please set GEMINI_API_KEY in Colab Secrets (ğŸ”‘ icon in left sidebar)")
    sys.exit(1)

# =============================================================================
# STEP 3: CONFIGURATION AND PATHS
# =============================================================================

# File paths configuration
BASE_DIR = '/content/drive/MyDrive/project_data/'
XML_FILE_PATHS = [
    os.path.join(BASE_DIR, 'Arboleda1119.xml'),
    os.path.join(BASE_DIR, 'Memorias Final.xml'),
    os.path.join(BASE_DIR, 'Vita1119.xml')
]

PDF_FILE_PATHS = [
    os.path.join(BASE_DIR, 'Transcription of Arboleda de los enfermos â€œTERESA DE CARTAGENAâ€.pdf'),
    os.path.join(BASE_DIR, 'Memorias.pdf')
]

OUTPUT_DIR = os.path.join(BASE_DIR, 'output_xml_annotations')
os.makedirs(OUTPUT_DIR, exist_ok=True)

# =============================================================================
# STEP 4: ENHANCED TAXONOMY DEFINITION
# =============================================================================

TAXONOMY = {
    "Genre": ["sermon", "vision", "consolatory_treatise", "vita", "confession",
              "prayer_devotional", "disputation", "performance", "music", "parable",
              "sacraments", "didactics", "scripture", "autobiography", "dialogue",
              "exemplum", "epistle", "theater_performance", "poetry", "music_laic",
              "testament_will", "speculum", "relacion", "genealogy", "testimony",
              "disputation_laic", "pastoral"],

    "Rhetoric": ["captatio", "colloquialism", "pathos", "logos", "ethos", "allegory",
                 "ekphrasis", "metaphor", "sign_symbol", "exegesis", "parallelism",
                 "didactics_rhet", "invective", "amplification", "anaphora", "antithesis",
                 "apostrophe", "exclamation", "polyptoton", "hypophora", "orality_literacy"],

    "Lexis": ["place", "person", "name", "gender", "building", "family", "authority",
              "body", "soul", "material", "nature", "animal", "time", "age", "food",
              "festivity", "latin", "sins", "virtues", "vice"],

    "Verb_Functions": ["affirm", "negate", "question", "exhort", "narrate", "describe",
                       "command", "promise", "lament", "praise", "blame", "warn",
                       "supplicate", "advise", "instruct", "prophesy", "invoke", "confess"],

    "Notes": ["hkbtext", "hkbobs", "intertext", "classical_sources", "biblical_sources",
              "contemporary_sources", "metatextual"]
}

# Create flat list of all valid subtypes for validation
ALL_VALID_SUBTYPES = set()
for subtypes in TAXONOMY.values():
    ALL_VALID_SUBTYPES.update(subtypes)

# Common subtype corrections
SUBTYPE_CORRECTIONS = {
    "capitatio": "captatio",
    "capitio": "captatio",
    "person_name": "person",
    "person_man": "person",
    "person_woman": "person",
    "description": "describe",
    "affirmation": "affirm",
    "negation": "negate",
    "questioning": "question",
    "exhortation": "exhort",
    "narration": "narrate",
    "commanding": "command",
    "promising": "promise",
    "lamenting": "lament",
    "praising": "praise",
    "blaming": "blame",
    "warning": "warn",
    "supplication": "supplicate",
    "advising": "advise",
    "instruction": "instruct",
    "instructing": "instruct",
    "prophecy": "prophesy",
    "prophesying": "prophesy",
    "invocation": "invoke",
    "invoking": "invoke",
    "confession": "confess",
    "confessing": "confess"
}

# =============================================================================
# STEP 5: DATA STRUCTURES
# =============================================================================

@dataclass
class SpanAnnotation:
    """Represents a single text annotation with span information."""
    text: str
    start: int
    end: int
    type: str
    subtype: str

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary format for JSON serialization."""
        return {
            "span": {
                "text": self.text,
                "start": self.start,
                "end": self.end
            },
            "type": self.type,
            "subtype": self.subtype
        }

@dataclass
class ChunkAnnotation:
    """Container for all annotations in a text chunk."""
    chunk_index: int
    original_text: str
    annotations: List[SpanAnnotation]
    processing_time: float

    def to_ml_format(self) -> List[Dict[str, Any]]:
        """Convert to ML training format."""
        return [ann.to_dict() for ann in self.annotations]

# =============================================================================
# STEP 6: ENHANCED TEXT EXTRACTION
# =============================================================================

def extract_text_from_xml(xml_file_path: str) -> str:
    """Extract text content from XML files with better handling."""
    try:
        with open(xml_file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        # Parse XML
        tree = ET.parse(xml_file_path)
        root = tree.getroot()

        text_parts = []

        # Extract text from all elements
        for elem in root.iter():
            if elem.text and elem.text.strip():
                text_parts.append(elem.text.strip())
            if elem.tail and elem.tail.strip():
                text_parts.append(elem.tail.strip())

        extracted_text = ' '.join(text_parts)
        print(f"  âœ“ Extracted {len(extracted_text)} characters from {os.path.basename(xml_file_path)}")
        return extracted_text

    except Exception as e:
        print(f"  âŒ Error extracting text from {xml_file_path}: {e}")
        return ""

def extract_text_from_pdf(pdf_file_path: str) -> str:
    """Extract text content from PDF files with better error handling."""
    try:
        with open(pdf_file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text_parts = []

            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_parts.append(page_text)
                except Exception as e:
                    print(f"    âš ï¸ Error reading page {page_num + 1}: {e}")
                    continue

            extracted_text = '\n'.join(text_parts)
            print(f"  âœ“ Extracted {len(extracted_text)} characters from {os.path.basename(pdf_file_path)}")
            return extracted_text

    except Exception as e:
        print(f"  âŒ Error extracting text from {pdf_file_path}: {e}")
        return ""

# =============================================================================
# STEP 7: ENHANCED TEXT CLEANING
# =============================================================================

def clean_extracted_text(text: str) -> str:
    """Comprehensive text cleaning for Medieval Spanish/Catalan texts."""
    if not text:
        return ""

    print(f"  ğŸ“ Original text length: {len(text)} characters")

    # Save original for comparison
    original_text = text

    # Step 1: Remove OCR artifacts and modern additions
    ocr_patterns = [
        # Page numbers and references
        r'\bPage\s+\d+\b',
        r'\bFolio\s+\d+[rv]?\b',
        r'\[p\.\s*\d+\]',
        r'\b\d{1,3}\s+of\s+\d{1,3}\b',
        r'^\s*\d+\s*$',
        r'^\s*[IVXLCDM]+\s*$',

        # Timestamps and dates
        r'\d{1,2}/\d{1,2}/\d{2,4}\s+\d{1,2}:\d{2}(?:\:\d{2})?\s*[AP]M',
        r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}',

        # URLs and email addresses
        r'https?://\S+',
        r'www\.\S+',
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',

        # OCR-specific artifacts
        r'\bOCRed\b',
        r'\bOCR\b',
        r'Scanned\s+by.*?Internet\s+Archive',
        r'Digitized\s+by.*?Google',
        r'Archive\.org',

        # Modern editorial additions
        r'Publicaciones.*?Montoto',
        r'Reflexiones.*?antiguo',
        r'Transcription\s+of\s+.*?TERESA\s+DE\s+CARTAGENA',
        r'TERESA\s+DE\s+CARTAGENA',

        # Formatting artifacts
        r'^\s*[-*â€¢]\s*$',
        r'^\s*[_=\-]{3,}\s*$',
        r'\[.*?\](?:\s*\[.*?\])*',  # Remove bracketed annotations

        # Headers and footers
        r'(?:^|\n)\s*(?:CHAPTER|CAPÃTULO|CAP\.)\s+[IVXLCDM\d]+\s*(?:\n|$)',
        r'(?:^|\n)\s*\d+\s*(?:\n|$)',
    ]

    # Apply OCR cleaning patterns
    for pattern in ocr_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)

    # Step 2: Normalize whitespace and punctuation
    text = re.sub(r'\s+', ' ', text)  # Multiple spaces to single
    text = re.sub(r'\n\s*\n+', '\n\n', text)  # Multiple newlines to double
    text = re.sub(r'([.!?])([A-ZÃÃ‰ÃÃ“ÃšÃ‘Ã‡])', r'\1 \2', text)  # Add space after punctuation
    text = re.sub(r'([,;:])([A-ZÃÃ‰ÃÃ“ÃšÃ‘Ã‡])', r'\1 \2', text)  # Add space after punctuation

    # Step 3: Fix common Medieval Spanish/Catalan character issues
    char_fixes = [
        (r'([aeiouÃ¡Ã©Ã­Ã³Ãº])n([aeiouÃ¡Ã©Ã­Ã³Ãº])', r'\1Ã±\2'),  # Fix missing tildes
        (r'([aeiouÃ¡Ã©Ã­Ã³Ãº])c([aeiouÃ¡Ã©Ã­Ã³Ãº])', r'\1Ã§\2'),  # Fix missing cedillas
        (r'(\w)j(\w)', r'\1i\2'),  # Medieval j to i
        (r'(\w)v(\w)', r'\1u\2'),  # Medieval v to u in middle of words
        (r'\bu(\w)', r'v\1'),  # u to v at beginning of words
    ]

    for pattern, replacement in char_fixes:
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)

    # Step 4: Remove excessive punctuation and clean up
    text = re.sub(r'[.]{3,}', '...', text)  # Multiple periods to ellipsis
    text = re.sub(r'[,]{2,}', ',', text)  # Multiple commas to single
    text = re.sub(r'[;]{2,}', ';', text)  # Multiple semicolons to single
    text = re.sub(r'[!]{2,}', '!', text)  # Multiple exclamations to single
    text = re.sub(r'[?]{2,}', '?', text)  # Multiple questions to single

    # Step 5: Final cleanup
    text = text.strip()
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # Triple+ newlines to double

    cleaned_length = len(text)
    removed_chars = len(original_text) - cleaned_length

    print(f"  âœ“ Cleaned text length: {cleaned_length} characters")
    print(f"  âœ“ Removed {removed_chars} characters ({removed_chars/len(original_text)*100:.1f}%)")

    # Show sample of cleaned text
    sample_length = min(200, len(text))
    print(f"  ğŸ“– Sample cleaned text: {repr(text[:sample_length])}")

    return text

# =============================================================================
# STEP 8: INTELLIGENT TEXT CHUNKING
# =============================================================================

def chunk_text_intelligently(text: str, max_chunk_size: int = 1200) -> List[str]:
    """Smart text chunking that respects sentence and paragraph boundaries."""
    if not text or len(text) <= max_chunk_size:
        return [text] if text else []

    chunks = []
    current_chunk = ""

    # Split by double newlines (paragraphs) first
    paragraphs = text.split('\n\n')

    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue

        # If paragraph is too long, split by sentences
        if len(paragraph) > max_chunk_size:
            # Split into sentences
            sentences = re.split(r'(?<=[.!?])\s+', paragraph)

            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue

                # If adding this sentence would exceed limit, start new chunk
                if len(current_chunk) + len(sentence) + 2 > max_chunk_size:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence
                else:
                    current_chunk += (" " + sentence) if current_chunk else sentence
        else:
            # If adding this paragraph would exceed limit, start new chunk
            if len(current_chunk) + len(paragraph) + 2 > max_chunk_size:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += ("\n\n" + paragraph) if current_chunk else paragraph

    # Add final chunk if it exists
    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    # Filter out very short chunks (less than 50 characters)
    chunks = [chunk for chunk in chunks if len(chunk) >= 50]

    print(f"  âœ‚ï¸ Created {len(chunks)} chunks")
    for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks
        print(f"    Chunk {i+1}: {len(chunk)} chars - {repr(chunk[:100])}")

    return chunks

# =============================================================================
# STEP 9: ENHANCED LLM ANNOTATION
# =============================================================================

def create_annotation_prompt(text_chunk: str) -> str:
    """Create a comprehensive prompt for the LLM with examples."""

    taxonomy_desc = ""
    for category, subtypes in TAXONOMY.items():
        taxonomy_desc += f"\n{category}:\n"
        for subtype in subtypes[:8]:  # Show first 8 to avoid token limit
            taxonomy_desc += f"  - {subtype}\n"
        if len(subtypes) > 8:
            taxonomy_desc += f"  ... and {len(subtypes) - 8} more\n"

    prompt = f"""You are an expert in Medieval Spanish and Catalan literature. Analyze the following text and identify meaningful spans that fit these categories:

{taxonomy_desc}

ANNOTATION RULES:
1. Only use the exact subtypes listed above
2. Identify spans of 2-50 words that clearly represent each category
3. Focus on significant literary, rhetorical, and semantic elements
4. Spans should not overlap
5. Return ONLY valid JSON array format

EXAMPLES:
- "Teresa de Cartagena" â†’ {{"span": {{"text": "Teresa de Cartagena", "start": 0, "end": 18}}, "type": "Lexis", "subtype": "person"}}
- "en el aÃ±o de 1425" â†’ {{"span": {{"text": "aÃ±o de 1425", "start": 6, "end": 17}}, "type": "Lexis", "subtype": "time"}}
- "Dios todopoderoso" â†’ {{"span": {{"text": "Dios todopoderoso", "start": 0, "end": 17}}, "type": "Lexis", "subtype": "authority"}}
- "convento de San Francisco" â†’ {{"span": {{"text": "convento de San Francisco", "start": 0, "end": 25}}, "type": "Lexis", "subtype": "building"}}

TEXT TO ANALYZE ({len(text_chunk)} characters):
{text_chunk}

Return JSON array with this exact format:
[{{"span": {{"text": "exact_text_from_above", "start": start_index, "end": end_index}}, "type": "Category", "subtype": "exact_subtype"}}]

JSON:"""

    return prompt

def correct_subtype(subtype: str) -> str:
    """Correct common subtype errors."""
    subtype_lower = subtype.lower()
    if subtype_lower in SUBTYPE_CORRECTIONS:
        return SUBTYPE_CORRECTIONS[subtype_lower]
    return subtype

def validate_annotation(annotation: Dict, original_text: str) -> Optional[SpanAnnotation]:
    """Validate and clean a single annotation."""
    try:
        # Extract span information
        span = annotation.get('span', {})
        if not isinstance(span, dict):
            return None

        text = span.get('text', '')
        start = span.get('start')
        end = span.get('end')
        ann_type = annotation.get('type', '')
        subtype = annotation.get('subtype', '')

        # Validate required fields
        if not all([text, isinstance(start, int), isinstance(end, int), ann_type, subtype]):
            return None

        # Validate indices
        if start < 0 or end > len(original_text) or start >= end:
            return None

        # Validate type exists in taxonomy
        if ann_type not in TAXONOMY:
            return None

        # Correct and validate subtype
        subtype = correct_subtype(subtype)
        if subtype not in TAXONOMY[ann_type]:
            return None

        # Validate text matches
        expected_text = original_text[start:end]
        if text.strip() != expected_text.strip():
            # Try to find the text in the original
            text_index = original_text.find(text.strip())
            if text_index != -1:
                start = text_index
                end = text_index + len(text.strip())
            else:
                return None

        return SpanAnnotation(
            text=text.strip(),
            start=start,
            end=end,
            type=ann_type,
            subtype=subtype
        )

    except Exception as e:
        print(f"    âš ï¸ Error validating annotation: {e}")
        return None

@backoff.on_exception(backoff.expo, Exception, max_tries=3)
def call_gemini_api(prompt: str) -> str:
    """Call Gemini API with retry logic."""
    model = genai.GenerativeModel('gemini-1.5-flash')
    response = model.generate_content(prompt)
    return response.text if response.text else ""

def parse_gemini_response(response_text: str, original_text: str) -> List[SpanAnnotation]:
    """Parse Gemini response and extract valid annotations."""
    if not response_text:
        return []

    # Extract JSON from response
    json_patterns = [
        r'\[[\s\S]*?\]',  # Standard JSON array
        r'```json\s*(\[[\s\S]*?\])\s*```',  # Code block
        r'json\s*(\[[\s\S]*?\])',  # Inline json
    ]

    json_text = None
    for pattern in json_patterns:
        match = re.search(pattern, response_text, re.DOTALL)
        if match:
            json_text = match.group(1) if match.groups() else match.group(0)
            break

    if not json_text:
        print("    âš ï¸ No JSON found in response")
        return []

    try:
        # Clean the JSON text
        json_text = json_text.strip()
        if not json_text.startswith('['):
            json_text = '[' + json_text
        if not json_text.endswith(']'):
            json_text = json_text + ']'

        # Parse JSON
        annotations_data = json.loads(json_text)
        if not isinstance(annotations_data, list):
            return []

        # Validate each annotation
        valid_annotations = []
        for ann_data in annotations_data:
            validated = validate_annotation(ann_data, original_text)
            if validated:
                valid_annotations.append(validated)

        return valid_annotations

    except json.JSONDecodeError as e:
        print(f"    âš ï¸ JSON parsing error: {e}")
        return []
    except Exception as e:
        print(f"    âš ï¸ Unexpected error parsing response: {e}")
        return []

def annotate_chunk(chunk: str, chunk_index: int) -> ChunkAnnotation:
    """Annotate a single text chunk."""
    start_time = time.time()

    try:
        # Create prompt
        prompt = create_annotation_prompt(chunk)

        # Call API
        response = call_gemini_api(prompt)

        # Parse response
        annotations = parse_gemini_response(response, chunk)

        processing_time = time.time() - start_time

        print(f"    âœ“ Found {len(annotations)} valid annotations in {processing_time:.1f}s")

        return ChunkAnnotation(
            chunk_index=chunk_index,
            original_text=chunk,
            annotations=annotations,
            processing_time=processing_time
        )

    except Exception as e:
        print(f"    âŒ Error annotating chunk {chunk_index}: {e}")
        return ChunkAnnotation(
            chunk_index=chunk_index,
            original_text=chunk,
            annotations=[],
            processing_time=time.time() - start_time
        )

# =============================================================================
# STEP 10: OUTPUT GENERATION
# =============================================================================

def generate_xml_output(chunk_annotations: List[ChunkAnnotation], source_files: List[str]) -> str:
    """Generate XML output with annotations."""
    # Create root element
    root = ET.Element('annotated_text')
    root.set('created', time.strftime('%Y-%m-%d %H:%M:%S'))
    root.set('total_chunks', str(len(chunk_annotations)))

    # Add metadata
    metadata = ET.SubElement(root, 'metadata')
    sources = ET.SubElement(metadata, 'sources')
    for i, source in enumerate(source_files):
        source_elem = ET.SubElement(sources, 'source')
        source_elem.set('id', f'src_{i}')
        source_elem.text = os.path.basename(source)

    # Add taxonomy
    taxonomy_elem = ET.SubElement(metadata, 'taxonomy')
    for category, subtypes in TAXONOMY.items():
        cat_elem = ET.SubElement(taxonomy_elem, 'category')
        cat_elem.set('name', category)
        for subtype in subtypes:
            sub_elem = ET.SubElement(cat_elem, 'subtype')
            sub_elem.text = subtype

    # Add annotated chunks
    body = ET.SubElement(root, 'body')
    total_annotations = 0

    for chunk_annotation in chunk_annotations:
        chunk_elem = ET.SubElement(body, 'chunk')
        chunk_elem.set('index', str(chunk_annotation.chunk_index))
        chunk_elem.set('length', str(len(chunk_annotation.original_text)))
        chunk_elem.set('annotations', str(len(chunk_annotation.annotations)))
        chunk_elem.set('processing_time', f"{chunk_annotation.processing_time:.2f}")

        if chunk_annotation.annotations:
            # Sort annotations by start position
            sorted_annotations = sorted(chunk_annotation.annotations, key=lambda x: x.start)

            # Build annotated text with proper XML structure
            text_elem = ET.SubElement(chunk_elem, 'text')

            # Create segments for annotated text
            last_end = 0

            for ann in sorted_annotations:
                # Add plain text before annotation
                if ann.start > last_end:
                    plain_text = chunk_annotation.original_text[last_end:ann.start]
                    if plain_text.strip():
                        plain_elem = ET.SubElement(text_elem, 'plain')
                        plain_elem.text = plain_text

                # Add annotated segment
                seg_elem = ET.SubElement(text_elem, 'seg')
                seg_elem.set('type', ann.type)
                seg_elem.set('subtype', ann.subtype)
                seg_elem.set('start', str(ann.start))
                seg_elem.set('end', str(ann.end))
                seg_elem.text = ann.text

                last_end = ann.end

            # Add remaining plain text
            if last_end < len(chunk_annotation.original_text):
                remaining_text = chunk_annotation.original_text[last_end:]
                if remaining_text.strip():
                    plain_elem = ET.SubElement(text_elem, 'plain')
                    plain_elem.text = remaining_text

            total_annotations += len(chunk_annotation.annotations)
        else:
            # No annotations, just add original text
            text_elem = ET.SubElement(chunk_elem, 'text')
            plain_elem = ET.SubElement(text_elem, 'plain')
            plain_elem.text = chunk_annotation.original_text

    # Add summary
    summary = ET.SubElement(metadata, 'summary')
    summary.set('total_annotations', str(total_annotations))

    # Calculate annotation statistics
    type_counts = Counter()
    subtype_counts = Counter()

    for chunk in chunk_annotations:
        for ann in chunk.annotations:
            type_counts[ann.type] += 1
            subtype_counts[f"{ann.type}:{ann.subtype}"] += 1

    # Add statistics
    stats_elem = ET.SubElement(summary, 'statistics')

    # Type statistics
    types_elem = ET.SubElement(stats_elem, 'types')
    for type_name, count in type_counts.most_common():
        type_stat = ET.SubElement(types_elem, 'type')
        type_stat.set('name', type_name)
        type_stat.set('count', str(count))

    # Subtype statistics
    subtypes_elem = ET.SubElement(stats_elem, 'subtypes')
    for subtype_name, count in subtype_counts.most_common():
        subtype_stat = ET.SubElement(subtypes_elem, 'subtype')
        subtype_stat.set('name', subtype_name)
        subtype_stat.set('count', str(count))

    # Convert to pretty XML
    rough_string = ET.tostring(root, encoding='unicode')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent='  ')

def generate_csv_output(chunk_annotations: List[ChunkAnnotation], source_files: List[str]) -> pd.DataFrame:
    """Generate CSV output with annotations."""
    rows = []

    for chunk_annotation in chunk_annotations:
        for ann in chunk_annotation.annotations:
            rows.append({
                'chunk_index': chunk_annotation.chunk_index,
                'annotation_text': ann.text,
                'start_position': ann.start,
                'end_position': ann.end,
                'type': ann.type,
                'subtype': ann.subtype,
                'text_length': len(ann.text),
                'chunk_length': len(chunk_annotation.original_text),
                'processing_time': chunk_annotation.processing_time,
                'source_files': '; '.join([os.path.basename(f) for f in source_files])
            })

    df = pd.DataFrame(rows)

    # Add summary statistics
    print(f"  ğŸ“Š CSV Statistics:")
    print(f"    Total annotations: {len(df)}")
    print(f"    Unique types: {df['type'].nunique()}")
    print(f"    Unique subtypes: {df['subtype'].nunique()}")
    print(f"    Type distribution:")
    for type_name, count in df['type'].value_counts().head(10).items():
        print(f"      {type_name}: {count}")

    return df

def generate_json_output(chunk_annotations: List[ChunkAnnotation], source_files: List[str]) -> Dict[str, Any]:
    """Generate JSON output for machine learning training."""
    output = {
        "metadata": {
            "created": time.strftime('%Y-%m-%d %H:%M:%S'),
            "total_chunks": len(chunk_annotations),
            "source_files": [os.path.basename(f) for f in source_files],
            "taxonomy": TAXONOMY
        },
        "annotations": []
    }

    total_annotations = 0

    for chunk_annotation in chunk_annotations:
        chunk_data = {
            "chunk_index": chunk_annotation.chunk_index,
            "original_text": chunk_annotation.original_text,
            "text_length": len(chunk_annotation.original_text),
            "processing_time": chunk_annotation.processing_time,
            "annotations": chunk_annotation.to_ml_format()
        }

        output["annotations"].append(chunk_data)
        total_annotations += len(chunk_annotation.annotations)

    # Add summary statistics
    output["metadata"]["total_annotations"] = total_annotations

    # Calculate statistics
    type_counts = Counter()
    subtype_counts = Counter()

    for chunk in chunk_annotations:
        for ann in chunk.annotations:
            type_counts[ann.type] += 1
            subtype_counts[f"{ann.type}:{ann.subtype}"] += 1

    output["metadata"]["statistics"] = {
        "type_counts": dict(type_counts),
        "subtype_counts": dict(subtype_counts)
    }

    return output

# =============================================================================
# STEP 11: MAIN EXECUTION PIPELINE
# =============================================================================

def main():
    """Main execution pipeline."""
    print("\n" + "="*60)
    print("ğŸ“š MEDIEVAL TEXT ANNOTATION PIPELINE")
    print("="*60)

    # Step 1: Extract text from all sources
    print("\nğŸ“– Step 1: Extracting text from source files...")
    all_texts = []
    processed_files = []

    # Process XML files
    for xml_path in XML_FILE_PATHS:
        if os.path.exists(xml_path):
            print(f"  ğŸ“„ Processing XML: {os.path.basename(xml_path)}")
            text = extract_text_from_xml(xml_path)
            if text:
                all_texts.append(text)
                processed_files.append(xml_path)
        else:
            print(f"  âš ï¸ File not found: {xml_path}")

    # Process PDF files
    for pdf_path in PDF_FILE_PATHS:
        if os.path.exists(pdf_path):
            print(f"  ğŸ“„ Processing PDF: {os.path.basename(pdf_path)}")
            text = extract_text_from_pdf(pdf_path)
            if text:
                all_texts.append(text)
                processed_files.append(pdf_path)
        else:
            print(f"  âš ï¸ File not found: {pdf_path}")

    if not all_texts:
        print("âŒ No text extracted from any files!")
        return

    print(f"âœ“ Successfully extracted text from {len(all_texts)} files")

    # Step 2: Clean and combine text
    print("\nğŸ§¹ Step 2: Cleaning and combining text...")
    cleaned_texts = []

    for i, text in enumerate(all_texts):
        print(f"  ğŸ”§ Cleaning text from file {i+1}/{len(all_texts)}")
        cleaned = clean_extracted_text(text)
        if cleaned:
            cleaned_texts.append(cleaned)

    if not cleaned_texts:
        print("âŒ No text remaining after cleaning!")
        return

    # Combine all cleaned texts
    combined_text = '\n\n'.join(cleaned_texts)
    print(f"âœ“ Combined text length: {len(combined_text)} characters")

    # Step 3: Chunk the text
    print("\nâœ‚ï¸ Step 3: Chunking text...")
    chunks = chunk_text_intelligently(combined_text, max_chunk_size=1500)

    if not chunks:
        print("âŒ No chunks created!")
        return

    # Ask user for number of chunks to process
    print(f"\nğŸ”¢ Total chunks available: {len(chunks)}")
    try:
        user_input = input("Enter number of chunks to process (or 'all' for all chunks): ").strip()
        if user_input.lower() == 'all':
            chunks_to_process = len(chunks)
        else:
            chunks_to_process = min(int(user_input), len(chunks))
    except (ValueError, KeyboardInterrupt):
        chunks_to_process = min(5, len(chunks))  # Default to 5 chunks

    print(f"âœ“ Processing {chunks_to_process} chunks")

    # Step 4: Annotate chunks
    print("\nğŸ·ï¸ Step 4: Annotating chunks...")
    chunk_annotations = []

    for i in range(chunks_to_process):
        print(f"  ğŸ“ Processing chunk {i+1}/{chunks_to_process}")
        annotation = annotate_chunk(chunks[i], i)
        chunk_annotations.append(annotation)

        # Add small delay to avoid rate limiting
        time.sleep(1)

    # Step 5: Generate outputs
    print("\nğŸ’¾ Step 5: Generating output files...")

    # Generate XML output
    print("  ğŸ“„ Generating XML output...")
    xml_content = generate_xml_output(chunk_annotations, processed_files)
    xml_path = os.path.join(OUTPUT_DIR, 'annotations.xml')

    with open(xml_path, 'w', encoding='utf-8') as f:
        f.write(xml_content)
    print(f"  âœ“ XML saved to: {xml_path}")

    # Generate CSV output
    print("  ğŸ“Š Generating CSV output...")
    csv_df = generate_csv_output(chunk_annotations, processed_files)
    csv_path = os.path.join(OUTPUT_DIR, 'annotations.csv')

    csv_df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"  âœ“ CSV saved to: {csv_path}")

    # Generate JSON output for ML training
    print("  ğŸ¤– Generating JSON output for ML training...")
    json_content = generate_json_output(chunk_annotations, processed_files)
    json_path = os.path.join(OUTPUT_DIR, 'ml_training_data.json')

    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_content, f, ensure_ascii=False, indent=2)
    print(f"  âœ“ JSON saved to: {json_path}")

    # Step 6: Final summary
    print("\nğŸ“‹ Final Summary:")
    print("=" * 40)

    total_annotations = sum(len(chunk.annotations) for chunk in chunk_annotations)
    total_processing_time = sum(chunk.processing_time for chunk in chunk_annotations)

    print(f"ğŸ“š Files processed: {len(processed_files)}")
    print(f"âœ‚ï¸ Chunks processed: {len(chunk_annotations)}")
    print(f"ğŸ·ï¸ Total annotations: {total_annotations}")
    print(f"â±ï¸ Total processing time: {total_processing_time:.1f} seconds")
    print(f"âš¡ Average time per chunk: {total_processing_time/len(chunk_annotations):.1f} seconds")

    # Show annotation type distribution
    type_counts = Counter()
    for chunk in chunk_annotations:
        for ann in chunk.annotations:
            type_counts[ann.type] += 1

    print(f"\nğŸ¯ Annotation Distribution:")
    for type_name, count in type_counts.most_common():
        percentage = (count / total_annotations) * 100
        print(f"  {type_name}: {count} ({percentage:.1f}%)")

    print(f"\nğŸ“ Output files saved to: {OUTPUT_DIR}")
    print(f"  ğŸ“„ XML: annotations.xml")
    print(f"  ğŸ“Š CSV: annotations.csv")
    print(f"  ğŸ¤– JSON: ml_training_data.json")

    print("\nğŸ‰ Pipeline completed successfully!")

# =============================================================================
# STEP 12: EXECUTION
# =============================================================================

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Pipeline interrupted by user")
    except Exception as e:
        print(f"\n\nâŒ Pipeline failed with error: {e}")
        print("Full traceback:")
        traceback.print_exc()
    finally:
        print("\nğŸ‘‹ Pipeline execution ended")