# -*- coding: utf-8 -*-
"""Clusterer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15bu6yhhOFFfEkMGFOi39x2BPXU8H6SQ-
"""
from google.colab import drive
drive.mount('/content/drive')

import os
import csv
import asyncio
import aiohttp
import time
from collections import deque # For efficient tracking of request timestamps
from google.colab import drive
from google.colab import userdata # For securely accessing API key from Colab Secrets

# --- Configuration ---
# IMPORTANT: You MUST mount your Google Drive first.
# The path below should be the path *within* your mounted Google Drive
# where your time period folders (e.g., 'TimePeriodA', 'TimePeriodB') are located.
# Example: If your folders are directly in 'My Drive/MyProject/Texts', set this to '/content/drive/MyDrive/MyProject/Texts'
BASE_DIR = '/content/drive/MyDrive/HistoryProject' # <--- **UPDATE THIS PATH**

# Name of the output CSV file (will be saved in the same directory as this script, or specify a full path)
OUTPUT_CSV_FILE = 'rhetorical_analysis_output.csv'

# Gemini API configuration
# Get the API key from Colab Secrets. Make sure you've added a secret named 'GEMINI_API_KEY'
# in Colab's Secrets panel with your actual Gemini API key.
API_KEY = userdata.get('apikey')
API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"

# Rate limit handling configuration for free tier
MAX_RETRIES = 5
INITIAL_RETRY_DELAY = 1  # seconds
MAX_RETRY_DELAY = 60 # seconds

# New pacing strategy: Target 15 requests per minute for the free tier
REQUESTS_PER_MINUTE_TARGET = 15
WINDOW_SECONDS = 60 # The time window for the RPM limit

# --- Helper Function for LLM Call ---
async def get_rhetorical_devices_from_llm(session, text_content):
    """
    Calls the Gemini LLM to identify rhetorical devices in the given text.
    Includes retry logic with exponential backoff for rate limit errors.
    Returns a string of devices separated by spaces, or an error string.
    """
    if not API_KEY:
        return "Error: GEMINI_API_KEY not found. Please set it in Colab Secrets."

    chat_history = []
    prompt = f"""
    Identify all rhetorical devices present in the following text.
    List them, separated by single spaces. Do not include any other text, punctuation (like commas), or formatting.
    If no rhetorical devices are found, output 'None'.

    Example:
    Text: 'The car was a beast, roaring down the highway. The wind whispered secrets through the trees.'
    Output:
    metaphor onomatopoeia personification alliteration

    Now, analyze this text:
    {text_content}
    """
    chat_history.append({"role": "user", "parts": [{"text": prompt}]})

    payload = {
        "contents": chat_history,
        "generationConfig": {
            # No responseSchema needed as we want a plain text string
        }
    }

    retries = 0
    current_delay = INITIAL_RETRY_DELAY

    while retries < MAX_RETRIES:
        try:
            async with session.post(f"{API_URL}?key={API_KEY}", json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    if result and result.get("candidates") and result["candidates"][0].get("content") and result["candidates"][0]["content"].get("parts"):
                        devices_string = result["candidates"][0]["content"]["parts"][0]["text"]
                        return devices_string.strip()
                    else:
                        print(f"LLM response structure unexpected: {result}")
                        return "Error: Unexpected LLM response structure."
                elif response.status == 429 or response.status >= 500: # Rate limit or server error
                    error_text = await response.text()
                    print(f"  Rate limit/Server error (Status {response.status}): {error_text}. Retrying in {current_delay:.2f} seconds...")
                    retries += 1
                    await asyncio.sleep(current_delay)
                    current_delay = min(current_delay * 2, MAX_RETRY_DELAY) # Exponential backoff
                else: # Other HTTP errors
                    error_text = await response.text()
                    print(f"  Error calling LLM: Status {response.status}, Response: {error_text}")
                    return f"Error: LLM call failed with status {response.status}. Details: {error_text}"
        except aiohttp.ClientError as e:
            print(f"  Network or client error during LLM call: {e}. Retrying in {current_delay:.2f} seconds...")
            retries += 1
            await asyncio.sleep(current_delay)
            current_delay = min(current_delay * 2, MAX_RETRY_DELAY)
        except Exception as e:
            print(f"  An unexpected error occurred during LLM call: {e}. Retrying in {current_delay:.2f} seconds...")
            retries += 1
            await asyncio.sleep(current_delay)
            current_delay = min(current_delay * 2, MAX_RETRY_DELAY)

    return f"Error: Failed to get rhetorical devices after {MAX_RETRIES} retries."

# --- Main Processing Function ---
async def process_text_files_and_create_csv(base_directory, output_csv):
    """
    Loops through folders in Google Drive, reads text files, calls LLM, and writes to CSV.
    Implements a burst-and-wait pacing strategy to adhere to RPM limits.
    """
    data_to_write = []
    csv_headers = ["text_name", "rhetoric_device_sequence", "time_period"]
    data_to_write.append(csv_headers)

    if not os.path.isdir(base_directory):
        print(f"Error: Base directory '{base_directory}' not found.")
        print("Please ensure your Google Drive is mounted and BASE_DIR is set correctly.")
        return

    # Deque to store timestamps of recent requests for rate limiting
    request_timestamps = deque()

    async with aiohttp.ClientSession() as session:
        for item_name in os.listdir(base_directory):
            item_path = os.path.join(base_directory, item_name)

            if os.path.isdir(item_path):
                time_period = item_name
                print(f"\nProcessing folder: {time_period}")

                for file_name in os.listdir(item_path):
                    if file_name.endswith(".txt"):
                        file_path = os.path.join(item_path, file_name)
                        print(f"  Processing file: {file_name}")

                        # --- Rate Limiting Logic (Burst and Wait) ---
                        current_time = time.time()
                        # Remove timestamps older than the window
                        while request_timestamps and request_timestamps[0] <= current_time - WINDOW_SECONDS:
                            request_timestamps.popleft()

                        # If we've hit the limit for the current window, wait
                        if len(request_timestamps) >= REQUESTS_PER_MINUTE_TARGET:
                            time_to_wait = WINDOW_SECONDS - (current_time - request_timestamps[0])
                            if time_to_wait > 0:
                                print(f"    Rate limit reached for this minute. Waiting for {time_to_wait:.2f} seconds...")
                                await asyncio.sleep(time_to_wait)
                                current_time = time.time() # Update current time after waiting

                        # Add current request timestamp
                        request_timestamps.append(current_time)
                        # --- End Rate Limiting Logic ---

                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                text_content = f.read()

                            rhetoric_devices = await get_rhetorical_devices_from_llm(session, text_content)
                            data_to_write.append([file_name, rhetoric_devices, time_period])

                        except FileNotFoundError:
                            print(f"    Error: File not found - {file_path}")
                        except Exception as e:
                            print(f"    An error occurred while processing {file_name}: {e}")

    try:
        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerows(data_to_write)
        print(f"\nCSV file '{output_csv}' created successfully!")
        print(f"You can find it in the Colab environment or download it from the files sidebar.")
    except Exception as e:
        print(f"Error writing CSV file: {e}")

await process_text_files_and_create_csv(BASE_DIR, OUTPUT_CSV_FILE)


# Rhetorical Device Clustering Pipeline (Updated: Cluster by Paragraph Entry)
# Dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score
import umap.umap_ as umap
from sklearn.preprocessing import StandardScaler

# --- 1. Load Pre-Annotated Data ---
# CSV should contain columns: ['sentence', 'device_sequence', 'time_period']
# 'sentence' contains a block of 1â€“3 paragraphs or a filename
# 'device_sequence' is a space-separated sequence like: "ethos pathos metaphor"
# 'time_period' indicates historical era but is not used for training

df = pd.read_csv("rhetorical_analysis_output.csv")

# --- 2. Convert device sequences to vectors ---
# Build a vocabulary of all rhetorical devices
df['rhetoric_device_sequence'] = df['rhetoric_device_sequence'].fillna('').astype(str)
all_devices = sorted({d for seq in df['rhetoric_device_sequence'] for d in str(seq).split()})
device_to_idx = {d: i for i, d in enumerate(all_devices)}

# Function to convert a sequence into a frequency vector
def sequence_to_vector(seq):
    vec = np.zeros(len(all_devices))
    for d in str(seq).split():
        if d in device_to_idx:
            vec[device_to_idx[d]] += 1
    return vec

# Convert each paragraph entry into a rhetorical vector
df['device_vector'] = df['rhetoric_device_sequence'].apply(sequence_to_vector)
X = np.vstack(df['device_vector'].values)
X_scaled = StandardScaler().fit_transform(X)

# --- 3. Dimensionality Reduction ---
reducer = umap.UMAP(random_state=42)
X_umap = reducer.fit_transform(X_scaled)

# --- 4. Clustering Function ---
def cluster_and_plot(X, X_2d, method='kmeans', n_clusters=4, eps=1.2, min_samples=3):
    if method == 'kmeans':
        model = KMeans(n_clusters=n_clusters, random_state=42)
    elif method == 'hierarchical':
        model = AgglomerativeClustering(n_clusters=n_clusters)
    elif method == 'dbscan':
        model = DBSCAN(eps=eps, min_samples=min_samples)
    else:
        raise ValueError("Unsupported clustering method")

    labels = model.fit_predict(X)

    # Plot
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=labels, palette='tab10')
    plt.title(f"{method.upper()} Clustering by Paragraph Entry (UMAP)")
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.legend(title="Cluster")
    plt.show()

    # Evaluation
    if len(set(labels)) > 1:
        sil_score = silhouette_score(X, labels)
        print(f"Silhouette Score ({method}):", sil_score)
    else:
        print(f"{method.upper()} did not form enough clusters to evaluate.")

    return labels

# --- 5. Run Clustering ---
labels_kmeans = cluster_and_plot(X_scaled, X_umap, method='kmeans', n_clusters=20)
labels_hier = cluster_and_plot(X_scaled, X_umap, method='hierarchical', n_clusters=10)
labels_db = cluster_and_plot(X_scaled, X_umap, method='dbscan', eps=1.0, min_samples=2)

# Save paragraph-level cluster assignments with time_period

df['cluster_kmeans'] = labels_kmeans
df['cluster_hierarchical'] = labels_hier
df['cluster_dbscan'] = labels_db

# Select relevant output columns for export
output_columns = ['text_name', 'rhetoric_device_sequence', 'time_period', 'cluster_kmeans', 'cluster_hierarchical', 'cluster_dbscan']
df[output_columns].to_csv("clustered_paragraphs.csv", index=False)

# --- 6. Visualize Device Frequencies by Cluster (KMeans) ---
def plot_2d_clusters(X_2d, labels, title="2D Cluster View"):
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=X_2d[:, 0], y=X_2d[:, 1], hue=labels, palette='tab10')
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")
    plt.title(title)
    plt.legend(title="Cluster")
    plt.tight_layout()
    plt.show()

# Optional: Call the simplified 2D visualization for KMeans
plot_2d_clusters(X_umap, labels_kmeans, title="KMeans Clusters on UMAP Projection")

# Call the function to visualize
plot_device_frequencies_by_cluster(df, 'cluster_kmeans')

import matplotlib.pyplot as plt
plt.plot(np.var(X_scaled, axis=0))
plt.title("Feature Variance Across Device Vectors")
plt.show()